{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "parliamentary-triangle",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:numpy Version is 1.19.4\n",
      "INFO:root:tensorflow Version is 2.4.0\n",
      "INFO:root:tensorflow_probability Version is 0.12.0\n",
      "INFO:root:\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten , Convolution2D, MaxPooling2D , Lambda, Conv2D, Activation,Concatenate, Input, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam , SGD , Adagrad\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, CSVLogger, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers , initializers, activations\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import dump, load\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import autokeras as ak\n",
    "import corner\n",
    "import os \n",
    "import sys\n",
    "import time\n",
    "import importlib\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "importlib.reload(logging)\n",
    "logging.basicConfig(level = logging.INFO)\n",
    "\n",
    "# limit GPU memory\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#   # Restrict TensorFlow to only use the first GPU\n",
    "try:\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "    gpus[0],\n",
    "    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=10000)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "except RuntimeError as e:\n",
    "# Visible devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "logging.info(\"numpy Version is {}\".format(np.__version__))\n",
    "logging.info(\"tensorflow Version is {}\".format(tf.keras.__version__))\n",
    "logging.info(\"tensorflow_probability Version is {}\".format(tfp.__version__))\n",
    "logging.info(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-public",
   "metadata": {},
   "source": [
    "Ref: https://agustinus.kristia.de/techblog/2016/12/17/conditional-vae/   \n",
    "Ref: https://keras.io/guides/customizing_what_happens_in_fit/   \n",
    "Ref: https://keras.io/examples/generative/vae/   \n",
    "Ref: https://github.com/hagabbar/VItamin/blob/c1ae6dfa27b8ab77193caacddd477fde0dece1c2/Models/VICI_inverse_model.py#L404   \n",
    "Ref: https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MultivariateNormalDiag    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "european-wednesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_transform(prediction_array: np.ndarray = 0)-> np.ndarray :\n",
    "    tmp = np.arctan2(prediction_array[:,0],prediction_array[:,1])\n",
    "    tmp = np.where(tmp >= 0 , tmp, 2*np.pi+tmp) \n",
    "    delta = tmp/np.pi*180     \n",
    "    return delta\n",
    "\n",
    "def feature_reshape(data,column=2):\n",
    "    tmp = data.reshape(len(data),column,36)\n",
    "    tmp = np.array([ element.T for element in tmp])\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "standing-election",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 5s, sys: 11.5 s, total: 3min 17s\n",
      "Wall time: 3min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "training_data = np.load(\"./nsi_data/sample_nsi_regression_1e7_v1.npz\")\n",
    "data_all = np.column_stack([training_data['ve_dune'][:,:36], training_data['vu_dune'][:,:36], training_data['vebar_dune'][:,:36], training_data['vubar_dune'][:,:36]])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data_all)\n",
    "\n",
    "target = np.column_stack([training_data[\"theta23\"], training_data[\"delta\"]/180*np.pi ])\n",
    "# target = target/180*np.pi \n",
    "\n",
    "x_train = data_all[:9000000]\n",
    "y_train = target[:9000000]\n",
    "y_train_delta = np.column_stack([np.sin(y_train[:,1]), np.cos(y_train[:,1])]) \n",
    "y_train_theta23 = y_train[:,0]\n",
    "\n",
    "x_test = data_all[9000000:]\n",
    "y_test = target[9000000:]\n",
    "y_test_delta = np.column_stack([np.sin(y_test[:,1]), np.cos(y_test[:,1])]) \n",
    "y_test_theta23 = y_test[:,0]\n",
    "\n",
    "x_train_poisson = np.random.poisson(x_train)/1000\n",
    "x_test_poisson = np.random.poisson(x_test)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-jersey",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-freedom",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 7\n",
    "\n",
    "\"\"\"\n",
    "Encoder 1 (parameter + spectrum)\n",
    "\"\"\"\n",
    "# parameter\n",
    "encoder_parameter_inputs = layers.Input(shape=(2,),name = 'encoder_parameter_inputs')\n",
    "x_parameter = layers.Dense(64, activation=\"relu\", name = 'dense_parameter_1')(encoder_parameter_inputs)\n",
    "x_parameter = layers.Dense(32, activation=\"relu\", name = 'dense_parameter_2')(x_parameter)\n",
    "x_parameter = layers.Dense(16, activation=\"relu\", name = 'dense_parameter_3')(x_parameter)\n",
    "\n",
    "# spectrum\n",
    "encoder_spectrum_inputs = layers.Input(shape=(144),name = 'encoder_spectrum_inputs')\n",
    "x_spectrum = layers.Dense(64, activation=\"relu\", name = 'dense_spectrum_1')(encoder_spectrum_inputs)\n",
    "x_spectrum = layers.Dense(32, activation=\"relu\", name = 'dense_spectrum_2')(x_spectrum)\n",
    "x_spectrum = layers.Dense(16, activation=\"relu\", name = 'dense_spectrum_3')(x_spectrum)\n",
    "\n",
    "# merged\n",
    "mergedOut_Encoder_1 = Concatenate()([x_parameter,x_spectrum])\n",
    "\n",
    "# sampling\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(mergedOut_Encoder_1)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(mergedOut_Encoder_1)\n",
    "# z = Sampling(name = 'Sampling_encoder')([z_mean, z_log_var])\n",
    "\n",
    "# build model\n",
    "encoder_1 = keras.Model([encoder_parameter_inputs, encoder_spectrum_inputs], [z_mean, z_log_var], name=\"encoder_1\")\n",
    "encoder_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-photographer",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Encoder 2 (spectrum)\n",
    "\"\"\"\n",
    "# spectrum\n",
    "encoder_spectrum_inputs = layers.Input(shape=(144,),name = 'encoder_spectrum_inputs')\n",
    "x_spectrum = layers.Dense(64, activation=\"relu\", name = 'dense_spectrum_1')(encoder_spectrum_inputs)\n",
    "x_spectrum = layers.Dense(32, activation=\"relu\", name = 'dense_spectrum_2')(x_spectrum)\n",
    "x_spectrum = layers.Dense(16, activation=\"relu\", name = 'dense_spectrum_3')(x_spectrum)\n",
    "\n",
    "# sampling\n",
    "z_mean = layers.Dense(10*latent_dim, name=\"z_mean\")(x_spectrum)\n",
    "z_log_var = layers.Dense(10*latent_dim, name=\"z_log_var\")(x_spectrum)\n",
    "z_weight = layers.Dense(10, name=\"z_weight\")(x_spectrum)\n",
    "# z = Sampling(name = 'Sampling_encoder')([z_mean, z_log_var])\n",
    "\n",
    "# build model\n",
    "encoder_2 = keras.Model(encoder_spectrum_inputs, [z_mean, z_log_var, z_weight], name=\"encoder_2\")\n",
    "encoder_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-terrorism",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Decoder Model (latent + spectrum)\n",
    "\"\"\"\n",
    "latent_dim_2 = 2\n",
    "\n",
    "decoder_latent_inputs = keras.Input(shape=(latent_dim,),name = 'decoder_latent_inputs')\n",
    "x_latent = layers.Dense(64, activation=\"relu\", name = 'dense_1')(decoder_latent_inputs)\n",
    "x_latent = layers.Dense(32, activation=\"relu\", name = 'dense_2')(x_latent)\n",
    "x_latent = layers.Dense(16, activation=\"relu\", name = 'dense_3')(x_latent)\n",
    "\n",
    "# spectrum\n",
    "decoder_spectrum_inputs = layers.Input(shape=(144,),name = 'decoder_spectrum_inputs')\n",
    "x_spectrum = layers.Dense(64, activation=\"relu\", name = 'dense_spectrum_1')(decoder_spectrum_inputs)\n",
    "x_spectrum = layers.Dense(32, activation=\"relu\", name = 'dense_spectrum_2')(x_spectrum)\n",
    "x_spectrum = layers.Dense(16, activation=\"relu\", name = 'dense_spectrum_3')(x_spectrum)\n",
    "\n",
    "# merged\n",
    "mergedOut_Decoder = Concatenate()([x_latent,x_spectrum])\n",
    "\n",
    "z2_mean = layers.Dense(latent_dim_2, name=\"z_mean\")(mergedOut_Decoder)\n",
    "z2_log_var = layers.Dense(latent_dim_2, name=\"z_log_var\")(mergedOut_Decoder)\n",
    "# z2 = Sampling(name = 'Sampling_decoder')([z2_mean, z2_log_var])\n",
    "\n",
    "decoder = keras.Model([decoder_latent_inputs, decoder_spectrum_inputs], [z2_mean, z2_log_var], name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-arbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ref: https://keras.io/guides/customizing_what_happens_in_fit/\n",
    "#Ref: https://keras.io/examples/generative/vae/\n",
    "#Ref: https://github.com/hagabbar/VItamin/blob/c1ae6dfa27b8ab77193caacddd477fde0dece1c2/Models/VICI_inverse_model.py#L404\n",
    "class CVAE(keras.Model):\n",
    "    def __init__(self, encoder1, encoder2, decoder, **kwargs):\n",
    "        super(CVAE, self).__init__(**kwargs)\n",
    "        self.encoder1 = encoder1  #(parameter + spectrum)\n",
    "        self.encoder2 = encoder2  #(spectrum)\n",
    "        self.decoder = decoder    #(latent + spectrum)\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "                ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            SMALL_CONSTANT = 1e-12 # necessary to prevent the division by zero in many operations \n",
    "            \n",
    "            # encoder1: \n",
    "            # input: [parameter, spectrum]\n",
    "            # output: [mean, log_var]\n",
    "            # mean.shape= (N, 15)  log_var.shape= (N, 15)   D(latent space) = 15\n",
    "            z1_mean, z1_log_var = self.encoder1(x)  #(parameter + spectrum)\n",
    "            \n",
    "            # GET q(z|x,y)   #(parameter + spectrum)\n",
    "            temp_var_q = SMALL_CONSTANT + tf.exp(z1_log_var)\n",
    "            mvn_q = tfp.distributions.MultivariateNormalDiag(\n",
    "                          loc=z1_mean,\n",
    "                          scale_diag=tf.sqrt(temp_var_q))\n",
    "            \n",
    "            z1 = mvn_q.sample()\n",
    "            \n",
    "            # encoder2: \n",
    "            # input: spectrum\n",
    "            # output: [mean, log_var, weight]\n",
    "            # mean.shape= (N, 10*15)  log_var.shape= (N, 10*15) weight.shape= (N, 10)  \n",
    "            # D(latent space)=15, D(mixture component)=10\n",
    "            z2_mean, z2_log_var, z2_weight = self.encoder2(x[1])  #(spectrum)\n",
    "\n",
    "            # mean->reshape= (N, 10, 15  log_var->reshape= (N, 10, 15) for create mixture model\n",
    "            z2_mean = tf.reshape(z2_mean, (-1, 10, 7))\n",
    "            z2_log_var = tf.reshape(z2_log_var, (-1, 10, 7))\n",
    "            z2_weight = tf.reshape(z2_weight, (-1, 10))\n",
    "\n",
    "            # Get r1(z|y) mixture model   #(spectrum)\n",
    "            temp_var_r1 = SMALL_CONSTANT + tf.exp(z2_log_var)\n",
    "            bimix_gauss = tfp.distributions.MixtureSameFamily(\n",
    "                          mixture_distribution=tfp.distributions.Categorical(logits=z2_weight),\n",
    "                          components_distribution=tfp.distributions.MultivariateNormalDiag(\n",
    "                          loc=z2_mean,\n",
    "                          scale_diag=tf.sqrt(temp_var_r1)))\n",
    "            \n",
    "            z2 = bimix_gauss.sample()\n",
    "            \n",
    "            # decoder: \n",
    "            # input: [latent, spectrum]\n",
    "            # output: [mean, log_var]\n",
    "            # mean.shape= (N, 2)  log_var.shape= (N, 2) \n",
    "            reconstruction_mean, reconstruction_var = self.decoder([z1, x[1]])      #(latent + spectrum)\n",
    "            \n",
    "            # GET r2(x|z,y)    #(latent + spectrum)\n",
    "            temp_var_r2 = SMALL_CONSTANT + tf.exp(reconstruction_var)\n",
    "            reconstruction_parameter = tfp.distributions.MultivariateNormalDiag(\n",
    "                                     loc=reconstruction_mean,\n",
    "                                     scale_diag= tf.sqrt(temp_var_r2))\n",
    "            \n",
    "            r2 = reconstruction_parameter.sample()\n",
    "        \n",
    "            log_q_q = mvn_q.log_prob(z1)\n",
    "            log_r1_q = bimix_gauss.log_prob(z1)               # evaluate the log prob of r1 at the q samples(z1)\n",
    "            kl_loss = tf.reduce_mean(log_q_q - log_r1_q)      # average over batch\n",
    "            \n",
    "            reconstruction_parameter_loss = reconstruction_parameter.log_prob(y)\n",
    "            reconstruction_loss = -1.0*tf.reduce_mean(reconstruction_parameter_loss)\n",
    "            \n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "            \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-kelly",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Training\n",
    "\"\"\"\n",
    "cvae = CVAE(encoder_1,encoder_2,decoder)\n",
    "cvae.compile(optimizer=keras.optimizers.Adam())\n",
    "# vae.compile(optimizer=keras.optimizers.Adadelta())\n",
    "\n",
    "check_list=[]\n",
    "csv_logger = CSVLogger(\"./Training_loss/CVAE_1DCNN_training_log_v2.csv\")\n",
    "check_list.append(csv_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-throw",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cvae.fit(x = [y_train, x_train_poisson],\n",
    "         y = y_train,\n",
    "         batch_size=1000,\n",
    "         epochs=100,\n",
    "         verbose=1,\n",
    "         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375c59fd-2439-4154-9d5b-2a8ecec2a7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './CVAE/DNN_{}'\n",
    "index = 1\n",
    "while os.path.isdir(path.format(index)):\n",
    "    index += 1\n",
    "path = path.format(index)\n",
    "cvae.encoder1.save(path + \"/encoder_1_test_v2.h5\")\n",
    "cvae.encoder2.save(path + \"/encoder_2_test_v2.h5\")\n",
    "cvae.decoder.save(path + \"/decoder_test_v2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4da294e0-7106-43e3-b303-124012d61c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './CVAE/DNN_{}'.format(1)\n",
    "encoder1 = load_model(path + \"/encoder_1_test_v2.h5\", compile=False)\n",
    "encoder2 = load_model(path + \"/encoder_2_test_v2.h5\", compile=False)\n",
    "decoder = load_model(path + \"/decoder_test_v2.h5\", compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85968fd1-ee2c-4cd8-a154-5bd1bda16ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:(1, 144)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_poisson_array = x_train[:1]\n",
    "logging.info(data_poisson_array.shape)\n",
    "\n",
    "mean, log_var, weight = encoder2.predict(data_poisson_array)\n",
    "# logging.info(mean.shape,log_var.shape,weight.shape)\n",
    "mean = tf.reshape(mean, (-1, 10, 7))\n",
    "log_var = tf.reshape(log_var, (-1, 10, 7))\n",
    "weight = tf.reshape(weight, (-1, 10))\n",
    "# logging.info(mean.shape,log_var.shape,weight.shape)\n",
    "\n",
    "SMALL_CONSTANT = 1e-12\n",
    "temp_var = SMALL_CONSTANT + tf.exp(log_var)\n",
    "test_sampling = tfp.distributions.MixtureSameFamily(\n",
    "              mixture_distribution=tfp.distributions.Categorical(logits=weight),\n",
    "              components_distribution=tfp.distributions.MultivariateNormalDiag(\n",
    "              loc=mean,\n",
    "              scale_diag=tf.sqrt(temp_var)))\n",
    "\n",
    "n_test= 100000\n",
    "prediction = []\n",
    "for i in range(n_test):\n",
    "    Z3 = test_sampling.sample()\n",
    "    reconstruction_mean, reconstruction_var = decoder.predict([Z3, data_poisson_array])\n",
    "    temp_var = SMALL_CONSTANT + tf.exp(reconstruction_var)\n",
    "    reconstruction_parameter = tfp.distributions.MultivariateNormalDiag(\n",
    "                             loc=reconstruction_mean,\n",
    "                             scale_diag= tf.sqrt(temp_var))\n",
    "    prediction.append(reconstruction_parameter.sample().numpy()[0])\n",
    "\n",
    "prediction = np.array(prediction)\n",
    "logging.info(prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784112d8-e272-4e07-abe6-28fe0b2a2608",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(prediction[:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-subscription",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd \n",
    "\n",
    "# fig, ax = plt.subplots(2,1, figsize=(8,16))\n",
    "\n",
    "# training_log = pd.read_csv(\"./Training_loss/CVAE_1DCNN_training_log.csv\")\n",
    "\n",
    "\n",
    "# ax[0].plot(training_log.index+1, training_log[\"reconstruction_loss\"], \"-\", color='g', label=\"reconstruction_loss\")\n",
    "# ax[0].plot(training_log.index+1, training_log[\"kl_loss\"], \"--\", color='lightgreen', label=\"kl_loss\")\n",
    "\n",
    "# ax[0].set_yscale(\"log\")\n",
    "\n",
    "# ax[0].tick_params(axis='x', labelsize=25)\n",
    "# ax[0].tick_params(axis='y', labelsize=25)\n",
    "\n",
    "# ax[0].legend(loc=\"best\",ncol=1,fontsize=20, edgecolor = \"w\",fancybox=False, framealpha=0)\n",
    "\n",
    "# ax[0].set_xlabel(\"Epoch\", fontsize=30,horizontalalignment='right',x=1) \n",
    "# ax[0].set_ylabel(\"Loss \", fontsize=30, horizontalalignment='right',y=1)\n",
    "\n",
    "\n",
    "# ax[1].plot(training_log.index+1, training_log[\"theta23_loss\"], \"-\", color='g', label=\"theta23_loss\")\n",
    "# ax[1].plot(training_log.index+1, training_log[\"delta_loss\"], \"--\", color='lightgreen', label=\"delta_loss\")\n",
    "\n",
    "# ax[1].set_yscale(\"log\")\n",
    "\n",
    "# ax[1].tick_params(axis='x', labelsize=25)\n",
    "# ax[1].tick_params(axis='y', labelsize=25)\n",
    "\n",
    "# ax[1].legend(loc=\"best\",ncol=1,fontsize=20, edgecolor = \"w\",fancybox=False, framealpha=0)\n",
    "\n",
    "# ax[1].set_xlabel(\"Epoch\", fontsize=30,horizontalalignment='right',x=1) \n",
    "# ax[1].set_ylabel(\"Loss \", fontsize=30, horizontalalignment='right',y=1)\n",
    "\n",
    "# # ax[0].set_ylim((0,0.02))\n",
    "# # ax[1].set_ylim((0,0.6))\n",
    "\n",
    "# # ax[0].set_xlim((80, 140))\n",
    "# # ax[1].set_xlim((80, 140))\n",
    "# # plt.yscale(\"log\")\n",
    "# # # plt.xlim((5))\n",
    "# # ax[0].set_ylim((0.015,0.035))\n",
    "# # ax[1].set_xlabel(\"Epoch\", fontsize=30,horizontalalignment='right',x=1) \n",
    "# # ax[1].set_ylabel(\"Val Loss \", fontsize=30, horizontalalignment='right',y=1)\n",
    "# # # plt.grid()\n",
    "\n",
    "# # ax[1].legend(loc=\"best\",ncol=1,fontsize=20, edgecolor = \"w\",fancybox=False, framealpha=0)\n",
    " \n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-stopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cvae.encoder1.save(\"./Model/CVAE_1DCNN_encoder_1_test_v2.h5\")\n",
    "# cvae.encoder2.save(\"./Model/CVAE_1DCNN_encoder_2_test_v2.h5\")\n",
    "# cvae.decoder.save(\"./Model/CVAE_1DCNN_decoder_test_v2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-flower",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cvae_encoder = load_model(\"./Model/VAE_1DCNN_encoder_v2.h5\", compile=False, custom_objects={\"Sampling\": Sampling})\n",
    "# cvae_decoder = load_model(\"./Model/VAE_1DCNN_decoder_v2.h5\", compile=False)\n",
    "\n",
    "cvae_encoder = encoder_2\n",
    "cvae_decoder = decoder\n",
    "\n",
    "test_data = training_data\n",
    "\n",
    "IO_or_NO = 0\n",
    "N = 1 \n",
    "\n",
    "if IO_or_NO == 0:\n",
    "    logging.info(\"NO\")\n",
    "    logging.info(\"True point: theta_23 = {:.2f} \\delta_cp = {:.2f}\".format(test_data['theta23'][0], test_data['delta'][0]))\n",
    "    true_theta_23, trua_delta = test_data['theta23'][0], test_data['delta'][0]\n",
    "\n",
    "\n",
    "    #VAE_1DCNN_v2\n",
    "    data_mid = np.column_stack([test_data[\"ve_dune\"][:,:36],  test_data[\"vu_dune\"][:,:36], test_data[\"vebar_dune\"][:,:36], test_data[\"vubar_dune\"][:,:36]])\n",
    "    data_NO_mid = data_mid[0]\n",
    "    logging.info(\"Test NO Data Shape:{}\".format(data_NO_mid.shape))\n",
    "\n",
    "    data_poisson = np.random.poisson(data_NO_mid, size = (N, len(data_NO_mid)))\n",
    "    data_poisson = scaler.transform(data_poisson)\n",
    "    data_poisson = feature_reshape(data_poisson,column=4)\n",
    "    test_data = data_poisson\n",
    "\n",
    "    if type(test_data) == list:\n",
    "        for i in range(len(test_data)):\n",
    "                logging.info(\"X [\"+str(i)+\"] shape {}\".format(test_data[i].shape))\n",
    "    else:\n",
    "        logging.info(\"X train/test shape: {}\".format(test_data.shape))\n",
    "    logging.info(\"\\n\")\n",
    "        \n",
    "else:\n",
    "    logging.info(\"IO\")\n",
    "    logging.info(\"True point: theta_23 = {:.2f} \\delta_cp = {:.2f}\".format(test_data['theta23'][1], test_data['delta'][1]))\n",
    "    true_theta_23, trua_delta = test_data['theta23'][1], test_data['delta'][1]\n",
    "\n",
    "\n",
    "    #VAE_1DCNN_v2\n",
    "    data_mid = np.column_stack([test_data[\"ve_dune\"][:,:36],  test_data[\"vu_dune\"][:,:36], test_data[\"vebar_dune\"][:,:36], test_data[\"vubar_dune\"][:,:36]])\n",
    "    data_IO_mid = data_mid[1]\n",
    "    logging.info(\"Test IO Data Shape:{}\".format(data_IO_mid.shape))\n",
    "\n",
    "    data_poisson = np.random.poisson(data_IO_mid, size = (N, len(data_IO_mid)))\n",
    "    data_poisson = scaler.transform(data_poisson)\n",
    "    data_poisson = feature_reshape(data_poisson,column=4)\n",
    "    test_data = data_poisson\n",
    "\n",
    "    if type(test_data) == list:\n",
    "        for i in range(len(test_data)):\n",
    "                logging.info(\"X [\"+str(i)+\"] shape {}\".format(test_data[i].shape))\n",
    "    else:\n",
    "        logging.info(\"X train/test shape: {}\".format(test_data.shape))\n",
    "    logging.info(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-highlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#CVAE\n",
    "\n",
    "data_poisson_array = data_poisson\n",
    "logging.info(data_poisson_array.shape)\n",
    "\n",
    "mean, log_var, weight = cvae_encoder.predict(data_poisson_array)\n",
    "# logging.info(mean.shape,log_var.shape,weight.shape)\n",
    "mean = tf.reshape(mean, (-1, 10, 15))\n",
    "log_var = tf.reshape(log_var, (-1, 10, 15))\n",
    "weight = tf.reshape(weight, (-1, 10))\n",
    "# logging.info(mean.shape,log_var.shape,weight.shape)\n",
    "\n",
    "temp_var = SMALL_CONSTANT + tf.exp(log_var)\n",
    "test_sampling = tfp.distributions.MixtureSameFamily(\n",
    "              mixture_distribution=tfp.distributions.Categorical(logits=weight),\n",
    "              components_distribution=tfp.distributions.MultivariateNormalDiag(\n",
    "              loc=mean,\n",
    "              scale_diag=tf.sqrt(temp_var)))\n",
    "\n",
    "\n",
    "\n",
    "n_test= int(1E+4)\n",
    "prediction = []\n",
    "for i in range(n_test):\n",
    "    Z3 = test_sampling.sample()\n",
    "    reconstruction_mean, reconstruction_var = cvae_decoder.predict([Z3, data_poisson_array])\n",
    "    temp_var = SMALL_CONSTANT + tf.exp(reconstruction_var)\n",
    "    reconstruction_parameter = tfp.distributions.MultivariateNormalDiag(\n",
    "                             loc=reconstruction_mean,\n",
    "                             scale_diag= tf.sqrt(temp_var))\n",
    "    prediction.append(reconstruction_parameter.sample().numpy()[0])\n",
    "\n",
    "prediction = np.array(prediction)\n",
    "logging.info(prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-walker",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "input_theta23 = np.array(prediction[:,0])\n",
    "# input_delta = np.array(prediction[:,1]/np.pi*180 )\n",
    "tmp = np.where(prediction[:,1] >= 0 , prediction[:,1], 2*np.pi+prediction[:,1]) \n",
    "input_delta = tmp/np.pi*180 \n",
    "# input_theta23 = np.array(prediction[0])\n",
    "# input_delta = np.array(angle_transform(prediction[1]))\n",
    "\n",
    "bins_theta23_globes = np.linspace(np.min(input_theta23),np.max(input_theta23),100)\n",
    "bins_delta_globes = np.linspace(np.min(input_delta),np.max(input_delta),100)\n",
    "\n",
    "# true_theta_23, trua_delta = y_test_theta23[index], angle_transform(np.array([y_test_delta[index]]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "likeliregion, xedges, yedges = np.histogram2d(input_theta23, input_delta, bins = [bins_theta23_globes, bins_delta_globes])\n",
    "likeliregion = likeliregion.T\n",
    "\n",
    "max_poi = np.where(likeliregion == likeliregion.max())\n",
    "max_theta23 = xedges[max_poi[1]][0]\n",
    "max_delta = yedges[max_poi[0]][0]\n",
    "print(\"Maximum: theta23 {}, delta {} \".format(xedges[max_poi[1]],yedges[max_poi[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-chair",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(input_theta23,density=1, bins=bins_theta23_globes)\n",
    "plt.hist(input_delta,density=1, bins=bins_delta_globes)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-department",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Import txtæª”\n",
    "\n",
    "if IO_or_NO == 0:\n",
    "    f_DUNE_NO = open(\"../Data/one_sigma_contour_DUNE_NO.dat\")\n",
    "    theta23_DUNE_NO = []\n",
    "    delta_cp_DUNE_NO = []\n",
    "    chi_DUNE_NO = []\n",
    "\n",
    "    while True:\n",
    "        s = f_DUNE_NO.readline().split()\n",
    "        array = []\n",
    "        for j in range(len(s)) :\n",
    "            array.append(float(s[j]))\n",
    "        if(len(array)>0):\n",
    "            theta23_DUNE_NO.append(array[0])\n",
    "            delta_cp_DUNE_NO.append(array[1]) \n",
    "            chi_DUNE_NO.append(array[2])\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    theta23_DUNE_NO = np.array(theta23_DUNE_NO)\n",
    "    delta_cp_DUNE_NO = np.array(delta_cp_DUNE_NO)\n",
    "    chi_DUNE_NO = np.array(chi_DUNE_NO)\n",
    "    \n",
    "    \n",
    "    # DUNE_NO\n",
    "    a0 = min(theta23_DUNE_NO)*100-300\n",
    "    b0 = max(theta23_DUNE_NO)*100 + 300\n",
    "    c0 = min(delta_cp_DUNE_NO)*10-60\n",
    "    d0 = max(delta_cp_DUNE_NO)*10+60\n",
    "    x0 = np.linspace(int(a0), int(b0), int((b0 - a0) + 1))/100\n",
    "    y0 = np.linspace(int(c0), int(d0), int(d0 - c0 + 1))/10\n",
    "    X0, Y0 = np.meshgrid(x0, y0)\n",
    "    Z0 = np.zeros((len(X0),len(X0[0])))\n",
    "\n",
    "    for i in range(len(theta23_DUNE_NO)):\n",
    "        a = np.where(X0 == theta23_DUNE_NO[i])[1][0]\n",
    "        b = np.where(Y0 == delta_cp_DUNE_NO[i])[0][0]\n",
    "        Z0[b][a] = 1\n",
    "            \n",
    "elif IO_or_NO == 1:   \n",
    "    f_DUNE_IO = open(\"../Data/one_sigma_contour_DUNE_IO.dat\")\n",
    "    theta23_DUNE_IO = []\n",
    "    delta_cp_DUNE_IO = []\n",
    "    chi_DUNE_IO = []\n",
    "\n",
    "    while True:\n",
    "        s = f_DUNE_IO.readline().split()\n",
    "        array = []\n",
    "        for j in range(len(s)) :\n",
    "            array.append(float(s[j])) \n",
    "        if(len(array)>0):\n",
    "            theta23_DUNE_IO.append(array[0])\n",
    "            delta_cp_DUNE_IO.append(array[1]) \n",
    "            chi_DUNE_IO.append(array[2])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    theta23_DUNE_IO = np.array(theta23_DUNE_IO)\n",
    "    delta_cp_DUNE_IO = np.array(delta_cp_DUNE_IO)\n",
    "    chi_DUNE_IO = np.array(chi_DUNE_IO)\n",
    "\n",
    "    # DUNE_IO\n",
    "    a0 = min(theta23_DUNE_IO)*100-300\n",
    "    b0 = max(theta23_DUNE_IO)*100 + 300\n",
    "    c0 = min(delta_cp_DUNE_IO)*10-60\n",
    "    d0 = max(delta_cp_DUNE_IO)*10+60\n",
    "    x0 = np.linspace(int(a0), int(b0), int((b0 - a0) + 1))/100\n",
    "    y0 = np.linspace(int(c0), int(d0), int(d0 - c0 + 1))/10\n",
    "    X0, Y0 = np.meshgrid(x0, y0)\n",
    "    Z0 = np.zeros((len(X0),len(X0[0])))\n",
    "\n",
    "    for i in range(len(theta23_DUNE_IO)):\n",
    "        a = np.where(X0 == theta23_DUNE_IO[i])[1][0]\n",
    "        b = np.where(Y0 == delta_cp_DUNE_IO[i])[0][0]\n",
    "        Z0[b][a] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-circus",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(15,15))\n",
    "\n",
    "#=========================== Upper Left Corner\n",
    "plot_axis = plt.subplot(2,2,1)\n",
    "\"\"\"\n",
    "ML\n",
    "\"\"\"\n",
    "plot_axis.hist(input_theta23,density=1,bins=bins_theta23_globes)\n",
    "if IO_or_NO == 0:\n",
    "    plot_axis.set_xlim((44,54))\n",
    "elif IO_or_NO == 1:\n",
    "    plot_axis.set_xlim((44,54))\n",
    "plot_axis.tick_params(axis='x', labelsize=25)\n",
    "plot_axis.tick_params(axis='y', labelsize=25)\n",
    "plot_axis.set_ylabel(r'density', fontsize=30)\n",
    "#===========================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#=========================== Down Right Corner\n",
    "plot_axis = plt.subplot(2,2,4)\n",
    "\"\"\"\n",
    "ML\n",
    "\"\"\"\n",
    "plot_axis.hist(input_delta,orientation=\"horizontal\",density=1, bins=bins_delta_globes)\n",
    "if IO_or_NO == 0:\n",
    "    plot_axis.set_ylim((145,245))\n",
    "elif IO_or_NO == 1:\n",
    "    plot_axis.set_ylim((236, 336))\n",
    "plot_axis.tick_params(axis='x', labelsize=25)\n",
    "plot_axis.tick_params(axis='y', labelsize=25)\n",
    "plot_axis.set_xlabel(r'density', fontsize=30)\n",
    "#===========================\n",
    "\n",
    "\n",
    "#=========================== Down Left Corner\n",
    "plot_axis = plt.subplot(2,2,3)\n",
    "\n",
    "# \"\"\"\n",
    "# ML (Poisson)\n",
    "# \"\"\"\n",
    "corner.hist2d(input_theta23, input_delta,\n",
    "                    levels=(0.68,),\n",
    "                    scale_hist=True,\n",
    "                    plot_datapoints=False,\n",
    "                    color='green',\n",
    "                    labels= [\"$\\\\theta_{23} $($^\\circ$)\", \"$\\delta_{cp} $($^\\circ$)\"],\n",
    "#                     range=[[44,54], [236, 336]],\n",
    "                    range=[[true_theta_23-5,true_theta_23+5], [trua_delta-50, trua_delta+50]],\n",
    "                    plot_contours = True,\n",
    "                    plot_density = False,\n",
    "                    fontsize=30,\n",
    "#                     bins = [bins_theta23_globes, bins_delta_globes],\n",
    "                    label_kwargs={\"fontsize\": 30},\n",
    "                    smooth=True\n",
    "                   )\n",
    "\n",
    "corner.hist2d(input_theta23, input_delta,\n",
    "                    levels=(0.95,),\n",
    "                    scale_hist=True,\n",
    "                    plot_datapoints=False,\n",
    "                    color='red',\n",
    "                    labels= [\"$\\\\theta_{23} $($^\\circ$)\", \"$\\delta_{cp} $($^\\circ$)\"],\n",
    "#                     range=[[44,54], [236, 336]],\n",
    "                    range=[[true_theta_23-5,true_theta_23+5], [trua_delta-50, trua_delta+50]],\n",
    "                    plot_contours = True,\n",
    "                    plot_density = False,\n",
    "                    fontsize=30,\n",
    "#                     bins = [bins_theta23_globes, bins_delta_globes],\n",
    "                    label_kwargs={\"fontsize\": 30},\n",
    "                    smooth=True\n",
    "                   )\n",
    "\n",
    "\n",
    "if IO_or_NO == 0:\n",
    "    plot_axis.scatter(true_theta_23, trua_delta, marker=\"d\", c=\"k\", s=10, label = \"True Point: $\\\\theta_{23}$=%.2f, $\\delta_{cp}$=%.2f\" %(true_theta_23, trua_delta))\n",
    "else:\n",
    "    plot_axis.scatter(true_theta_23, trua_delta, marker=\"d\", c=\"k\", s=10, label = \"True Point: $\\\\theta_{23}$=%.2f, $\\delta_{cp}$=%.2f\" %(true_theta_23, trua_delta))\n",
    "\n",
    "plot_axis.scatter(xedges[max_poi[1]],yedges[max_poi[0]+1], marker=\"s\", c=\"r\", s=10, label = \"Maximum Point: $\\\\theta_{23}$=%.2f, $\\delta_{cp}$=%.2f\" %(xedges[max_poi[1]][0], yedges[max_poi[0]+1][0]))\n",
    "# plot_axis.scatter(xedges_asimov[max_poi_asimov[1]],yedges_asimov[max_poi_asimov[0]], marker=\"d\", c=\"blue\", s=5, label = \"maximum point: $\\\\theta_{23}$=%.2f, $\\delta_{cp}$=%.2f\" %(xedges_asimov[max_poi_asimov[1]][0], yedges_asimov[max_poi_asimov[0]][0]))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "globes\n",
    "\"\"\"\n",
    "DU = plot_axis.contour(X0, Y0, Z0, 0, colors='green', linestyles=\"--\", linewidths=1 )\n",
    "DU.collections[0].set_label(\"GLOBES (1$\\sigma$)\")\n",
    "\n",
    "# DU_IO = plot_axis.contour(X1, Y1, Z1, 0, colors='green', linestyles=\"--\", linewidths=1 )\n",
    "# DU_IO.collections[0].set_label(\"GLOBES (1$\\sigma$)\")\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# ML (Poisson)\n",
    "# \"\"\"\n",
    "# CS_1_sigma = plt.contour(xaxis, yaxis, one_sigma_region_boundary, 0, colors='green', linestyles=\"-\", linewidths=1)\n",
    "# CS_2_sigma = plt.contour(xaxis, yaxis, two_sigma_region_boundary, 0, colors='red', linestyles=\"-\", linewidths=1)\n",
    "# CS_1_sigma.collections[0].set_label(\"ML Estimate (1$\\sigma$)\")\n",
    "# CS_2_sigma.collections[0].set_label(\"ML Estimate (2$\\sigma$)\")\n",
    "\n",
    "\n",
    "plot_axis.set_xlabel(r'$\\theta_{23} $($^\\circ$)', fontsize=30)\n",
    "plot_axis.set_ylabel(r'$\\delta_{cp} $($^\\circ$)', fontsize=30)\n",
    "if IO_or_NO == 0:\n",
    "    plot_axis.set_ylim((145,245))\n",
    "    plot_axis.set_xlim((44,54))\n",
    "elif IO_or_NO == 1:\n",
    "    plot_axis.set_ylim((236, 336))\n",
    "    plot_axis.set_xlim((44,54))\n",
    "else:\n",
    "    raise ValueError(\"aaaa\")\n",
    "    \n",
    "    \n",
    "plot_axis.tick_params(axis='x', labelsize=25)\n",
    "plot_axis.tick_params(axis='y', labelsize=25)\n",
    "#===========================\n",
    "                              \n",
    "    \n",
    "\n",
    "#=========================== Whole Figure Setting\n",
    "\n",
    "if IO_or_NO == 0:\n",
    "    plt.text(x=54.5,y=355, s=\"Normal Ordering\", fontsize=25 )\n",
    "#     plt.text(x=54.5,y=340, s= str(index)+\" Training\", fontsize=25 )\n",
    "    \n",
    "elif IO_or_NO == 1:\n",
    "    plt.text(x=54.5,y=440, s=\"Inverse Ordering\", fontsize=25 )\n",
    "#     plt.text(x=54.5,y=430, s= str(index)+\" Training\", fontsize=25 ) \n",
    "    \n",
    "# plt.subplots_adjust(wspace=0.15, hspace=0.01)\n",
    "plt.legend(bbox_to_anchor=(2.3, 1.7), ncol=1,fontsize=20, markerscale=4, edgecolor = \"w\",fancybox=False, framealpha=0)\n",
    "# plt.savefig(\"./Plots/\"+str(experiment)+\"_\"+str(\"to_5GeV\")+\"_\"+str(\"IO_\")+str(index)+\".pdf\", transparent=True, bbox_inches='tight') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a543d6-ba3a-425c-824b-b6418721ffa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
